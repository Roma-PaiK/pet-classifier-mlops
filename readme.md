# Pet Classifier MLOps Project ğŸ¾

An end-to-end MLOps pipeline for classifying images of Cats and Dogs. This project demonstrates model development, experiment tracking, data versioning, containerization, and automated CI/CD deployment.

---

## ğŸ“ Project Structure

```text
pet-classifier-mlops/
â”œâ”€â”€ .github/workflows/      # CI Pipeline definitions (M3)
â”œâ”€â”€ data/                   # Raw and processed datasets (Tracked via DVC)
â”œâ”€â”€ models/                 # Serialized trained models (.h5) (Tracked via DVC)
â”œâ”€â”€ notebooks/              # Jupyter notebooks for model training and EDA
â”œâ”€â”€ mlruns/                 # Local MLflow experiment tracking logs
â”œâ”€â”€ src/                    # Reusable preprocessing/inference helpers (M3)
â”œâ”€â”€ tests/                  # Unit tests (pytest) (M3)
â”œâ”€â”€ scripts/                # Smoke test + post-deploy evaluation scripts (M4/M5)
â”œâ”€â”€ app.py                  # FastAPI inference service
â”œâ”€â”€ docker-compose.yml      # Local deployment using Docker Compose (M4)
â”œâ”€â”€ Dockerfile              # Containerization blueprint
â”œâ”€â”€ requirements.txt        # Pinned dependencies for the production environment
â”œâ”€â”€ .dvc/                   # DVC pipeline and tracking config
â”œâ”€â”€ .gitignore              # Git ignore rules for heavy files
â””â”€â”€ README.md               # Project documentation
```

---

## âœ… Completed Milestones

### M1: Model Development & Experiment Tracking

- Built a baseline Convolutional Neural Network (CNN) using TensorFlow 2.19.0  
- Versioned the raw dataset (PetImages) and serialized model (`baseline_cnn.h5`) using DVC  
- Tracked experiments, hyperparameters, and artifacts using MLflow  

### M2: Model Packaging & Containerization

- Packaged the trained model into a REST API using FastAPI  
- Implemented `/health` and `/predict` endpoints  
- Pinned strict environment dependencies in `requirements.txt`  
- Containerized the inference service using Docker  

### M3: CI Pipeline (Automated Testing + Build Verification)
- Added unit tests using pytest:
- Image preprocessing validation (shape/type)
- Inference post-processing logic validation (thresholding)
- Added GitHub Actions CI workflow to run on every PR / push:
- Installs dependencies
- Runs unit tests
- Builds Docker image to ensure container build does not break
- Files added (M3):
    - .github/workflows/ci.yml
    - src/preprocess.py, src/inference.py
    - tests/test_preprocess.py, tests/test_inference.py

### M4: Deployment (Docker Compose + Smoke Test)
- Added docker-compose.yml for repeatable local deployment
- Added a smoke test script to validate deployment:
- Calls /health
- Calls /metrics (available after M5 is enabled)
- Files added (M4):
    - docker-compose.yml
    - scripts/smoke_test.py

### M5: Monitoring (Logs + Metrics + Post-deploy Evaluation)
- Added basic monitoring capability:
- Prediction logs including label, confidence, and latency
- /metrics endpoint exposing lightweight counters (request count, failures, last latency)
- Added a post-deploy evaluation script:
- Sends a small labeled dataset to the deployed API
- Computes accuracy and exports results to CSV
- Files added (M5):
    - scripts/eval_post_deploy.py

---

## ğŸ› ï¸ How to Run Locally

### 1. Prerequisites

Ensure the following tools are installed:

- Git  
- DVC  
- Docker Desktop  
- Python 3.10+  

---

### 2. Clone and Setup

Clone the repository and pull the heavy files (data and models) from the DVC remote storage.

```bash
git clone https://github.com/YourUsername/pet-classifier-mlops.git
cd pet-classifier-mlops

# Pull dataset and model files tracked by DVC
dvc pull
```

---

### 3. Run the Containerized API

Build the Docker image and start the FastAPI server on port 8000:

```bash
docker build -t pet-classifier-api .
docker run -p 8000:8000 pet-classifier-api
```

---

### 4. Test the Endpoints

#### Health Check

```bash
curl http://localhost:8000/health
```

#### Prediction

Option 1: Open Swagger UI in your browser:

```
http://localhost:8000/docs
```

Option 2: Use cURL

```bash
curl -X POST -F "file=@test_image.jpg" http://localhost:8000/predict
```

---


## ğŸš€ How to Run M3, M4, and M5

### M3 â€” CI (Run Tests Locally)

Run unit tests using `pytest`:

```bash
pip install -r requirements.txt
pip install pytest
pytest -q
```

### M3 â€” Verify CI on GitHub
- Push changes to GitHub
- Go to Actions tab
- Confirm workflow success (tests + docker build)

### M4 â€” Deployment (Docker Compose)
```bash
docker compose up --build -d
```

Verify service:
```bash
curl http://localhost:8000/health
```

### M4 â€” Smoke Test
```bash
python scripts/smoke_test.py http://localhost:8000
```

### M5 â€” Monitoring
Check metrics:
```bash
curl http://localhost:8000/metrics
```

View logs:
```bash
docker compose logs -f api
```

### M5 â€” Post-deploy Evaluation

Prepare dataset:
```text
eval_data/
  Cat/
  Dog/
```

Run evaluation:
```bash
python scripts/eval_post_deploy.py \
  --base-url http://localhost:8000 \
  --data eval_data
  ```
  Output:
- Prints accuracy
- Generates eval_results.csv